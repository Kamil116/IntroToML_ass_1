{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IML Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Kamil Mirgasimov\n",
    "\n",
    "\n",
    "## Mail: k.mirgasimov@innopolis.university\n",
    "\n",
    "\n",
    "## Group: B22-SD-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code style policy \n",
    "\n",
    "We expect you to follow https://peps.python.org/pep-0008/ Python standart style and will reduce your points if you don't. Also we ask you to comment your code when it's needed (logical blocks, function declaration, loops), however over-documentation is the evil.\n",
    "\n",
    "Example of nice code style (no need to run this cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the sum of parameters\n",
    "# @param my_param1 - here I explain what this parameter means\n",
    "# @param my_param2 - here I explain what this parameter means\n",
    "# @return - result of func if it's not void\n",
    "def my_func(my_param1: int, my_param2: int):\n",
    "    return my_param1 + my_param2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few lines only, but they are represents important logical blocks, thus you should explain what their purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my_training_package import my_regression, my_loader\n",
    "\n",
    "# # Data loading\n",
    "# x, y = my_loader.load(\"some.csv\")\n",
    "\n",
    "# # Training\n",
    "# reg = my_regression()\n",
    "# reg.train(x,y)\n",
    "\n",
    "# # Evaluation on the same data set\n",
    "# y_pred = reg.evaluation(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of too detailed and meaningless commenting that is not welcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Import numpy package\n",
    "import numpy as np\n",
    "# This is variable x\n",
    "x = 5\n",
    "# This is variable y\n",
    "y = 10\n",
    "# Print x\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we belive in your programming common sense :) The purpose of clear code style is fast and smooth grading of your implementation and checking that you understand ML concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Linear Regression\n",
    "#### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# TODO Write your code here\n",
    "df = pd.read_csv('train_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\\validation splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO Write your code here\n",
    "x = df.drop(['y'], axis=1)\n",
    "y = df.loc[:,\"y\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Declare and train a linear regression model\n",
    "# TODO Write your code here\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "# Prediction by model on the validation set\n",
    "# TODO Write your code here\n",
    "y_pred_lr = linear_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model prediction & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 3826.726825199097\n",
      "Root mean squared error: 61.86054336327072\n",
      "Mean absolute error: 51.482778038978836\n",
      "R-2 score: 0.877207103326709\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print MSE, RMSE, MAE and R2 score\n",
    "def print_metrics(y_actual, y_pred):\n",
    "    mse = metrics.mean_squared_error(y_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = metrics.mean_absolute_error(y_actual, y_pred)\n",
    "    r_2 = metrics.r2_score(y_actual, y_pred)\n",
    "    print('Mean squared error:', mse)\n",
    "    print('Root mean squared error:', rmse)\n",
    "    print('Mean absolute error:', mae)\n",
    "    print('R-2 score:', r_2)\n",
    "\n",
    "\n",
    "print_metrics(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 2 Polynomial Regression\n",
    "#### Constructing the polynomial regression pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "polynomial_degree = PolynomialFeatures(degree=2)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynom\", polynomial_degree),\n",
    "                         (\"linear_regression\", linear_regression)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the degree hyper-parameter using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'polynom__degree': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search score: [ -8.52888376  -8.1584728  -25.57196777 -19.88560918 -27.7930192 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Declare a GridSearch instance \n",
    "# TODO Write your code here\n",
    "search = GridSearchCV(estimator=pipeline, param_grid={'polynom__degree': range(2, 6)}, cv=8, scoring ='neg_mean_squared_error')\n",
    "\n",
    "# Train the GridSearch\n",
    "# TODO Write your code here\n",
    "grid_search = search.fit(x_train, y_train)\n",
    "y_pred = grid_search.best_estimator_.predict(x_test)\n",
    "\n",
    "# Find the optimum degrees\n",
    "# TODO Write your code here\n",
    "print(f\"Best parameter: {grid_search.best_params_}\" )\n",
    "\n",
    "# Print the GridSearchCV score\n",
    "# TODO Write your code here\n",
    "# TODO\n",
    "print(f\"search score: {cross_val_score(grid_search, x_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.1744904325824161\n",
      "Root mean squared error: 1.083739098022405\n",
      "Mean absolute error: 0.7518532309274343\n",
      "R-2 score: 0.9999623126789762\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_actual=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Save the GridSearch model for evaluation\n",
    "filename = 'poly_optimized_model.sav'\n",
    "pickle.dump(search, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Determine the linear dependent features\n",
    "\n",
    "Use the following code cell to determine a pair of features from the training dataset that are correlated to each other. Explain your choise in the markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for X_1 and X_2: 0.3933888206271863\n",
      "Correlation for X_1 and X_3: 0.6964506177005838\n",
      "Correlation for X_1 and X_4: 0.7162306612508836\n",
      "Correlation for X_2 and X_3: -0.3689174252604542\n",
      "Correlation for X_2 and X_4: 0.9026708037739416\n",
      "Correlation for X_3 and X_4: -0.0017733618103473502\n"
     ]
    }
   ],
   "source": [
    "# TODO Write your code here\n",
    "df_modified = df.drop(df.columns[[0, -1]], axis=1)\n",
    "for i in range(len(df_modified.columns)):\n",
    "    for j in range(i + 1, len(df_modified.columns)):\n",
    "        row_i = df_modified.iloc[i]\n",
    "        row_j = df_modified.iloc[j]\n",
    "        correlation = row_i.corr(row_j)\n",
    "        print(f'Correlation for {df_modified.columns[i]} and {df_modified.columns[j]}: {correlation}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data processing\n",
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the dataset  and removing 2 redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Seed Pokémon\n",
       "1             Seed Pokémon\n",
       "2             Seed Pokémon\n",
       "3           Lizard Pokémon\n",
       "4            Flame Pokémon\n",
       "              ...         \n",
       "796         Launch Pokémon\n",
       "797    Drawn Sword Pokémon\n",
       "798      Junkivore Pokémon\n",
       "799          Prism Pokémon\n",
       "800     Artificial Pokémon\n",
       "Name: classification, Length: 801, dtype: object"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Write your code here\n",
    "df = pd.read_csv('pokemon_modified.csv')\n",
    "df.head()\n",
    "df.loc[:,'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data\n",
    "Use random_state = 123, stratify, and set test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# TODO Write your code here\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO Write your code here\n",
    "X_train, X_test, y_train, y_test =..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataset is balanced or not and comment on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define a SimpleImputer instance\n",
    "# TODO Write your code here\n",
    "imputer = ...\n",
    "\n",
    "# Apply the imputer\n",
    "# TODO Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double check that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "# Define a scaler instance from one of the above\n",
    "# TODO Write your code here\n",
    "scaler = ...\n",
    "\n",
    "# Apply the scaler on both train and test features\n",
    "# TODO Write your code here\n",
    "x_train = ...\n",
    "x_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Correlation matrix</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there highly co-related features in the dataset? Is it a problem? Explain in the markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "# Plot the correlation matrix\n",
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model fitting and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate and print classification metrics: accuracy, precision, recall, and F1 score \n",
    "# TODO Write your code here\n",
    "def print_clf_metrics(y_actual, y_pred ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify GridSearchCV as in intruction\n",
    "# TODO Write your code here\n",
    "parameters = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Declare and train logistic regression inside GridSearchCV with the parameters above\n",
    "# Set max_iter=1000 in LR constructor\n",
    "# TODO Write your code here\n",
    "lr_clf_gs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuned Hyperparameters :\",)\n",
    "print(\"Accuracy :\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a LR with the best params and Evaluate the LR with the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "lr_clf = ...\n",
    "lr_y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_clf_metrics(y_test, lr_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the top 5 most influencing features and the top 5 ignored features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Declare and train knn inside GridSearchCV\n",
    "# TODO Write your code here\n",
    "param_grid = ...\n",
    "knn_clf_gs = ...\n",
    "\n",
    "\n",
    "print(\"Tuned Hyperparameters :\", )\n",
    "print(\"Accuracy :\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a KNN model with the best params and Evaluate the KNN with the best params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "knn_clf = ...\n",
    "knn_y_pred = ...\n",
    "print_clf_metrics(y_test, knn_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting GNB to the data and evaluating on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Declare and train GaussianNB. No hyperparameters tuning \n",
    "# TODO Write your code here\n",
    "gauss_nb_clf = ...\n",
    "gauss_y_pred = ...\n",
    "\n",
    "print_clf_metrics(y_test, gauss_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which metric is most appropriate for this task and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the 3 classifiers in terms of accuracy, precision, recall and F1-score.\n",
    "What is the best model for this task? Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO Write your code here\n",
    "train_data = ...\n",
    "\n",
    "test_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "# TODO Write your code here\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "print(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Plot the data using the pairplot in sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# TODO Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit LR to the training dataset using OVR and evaluate on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "ovr_lr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit LR to the training dataset using multinomial and evaluate on the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "multi_lr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using gridsearch to tune the C value and multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "params = ...\n",
    "grid_search_clf = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuned Hyperparameters :\")\n",
    "print(\"Accuracy :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on why one multi_class technique was better than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LR with the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "multi_lr = ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "# TODO Write your code here\n",
    "multi_lr = ...\n",
    "\n",
    "plot_decision_regions()\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic Regression decision boundary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on the decision boundary, do you think this is a good model or not? and based on what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
